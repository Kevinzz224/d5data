{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD4q_bFhvkT_"
      },
      "source": [
        "## Take Home Test: Reformat a Public Dataset for LLM Training\n",
        "\n",
        "### Objective\n",
        "\n",
        "The goal of this task is to prepare public datasets for more effective use in training and fine-tuning Large Language Models (LLMs). You are required to reformat a specific subset of a public dataset into a structured, consistent format to facilitate its usability.\n",
        "\n",
        "### Detailed Instructions\n",
        "\n",
        "#### 1. Dataset Selection and Preparation\n",
        "\n",
        "- **Dataset:** You are assigned the `Headline` subset of the [AdaptLLM/finance-tasks](https://huggingface.co/datasets/AdaptLLM/finance-tasks) dataset.\n",
        "\n",
        "- **Task Description:** Each entry in the `input` column contains multiple \"Yes\" or \"No\" questions alongside their respective answers. Your task is to:\n",
        "\n",
        "  - Develop a Python script to parse and separate each question and its answer from the entry.\n",
        "  - Save each question-answer pair in a structured JSON format as follows:\n",
        "    ```json\n",
        "    {\n",
        "      \"id\": \"<unique_identifier>\",\n",
        "      \"Question\": \"<question_text>\",\n",
        "      \"Answer\": \"<answer_text>\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "  - You are encouraged to introduce additional attributes if needed to preserve the integrity and completeness of the information. Adding relevant tag information is strongly recommended.\n",
        "- **Automation Requirement:** The task must be completed using Python. Manual editing or data manipulation is strictly prohibited. Your script should efficiently handle variations in data format within the column.\n",
        "\n",
        "#### 2. Deliverables\n",
        "\n",
        "- **Reformatted Dataset:** Provide the schema of the final format you adopted for saving the results.\n",
        "- **Transformation Code:** Submit the complete code used for converting the dataset into the designated format.\n",
        "- **Statistics:** Report the total number of question-answer pairs extracted from the dataset.\n",
        "- **Performance Metrics:** Document the time taken to complete the dataset cleanup and transformation process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-YRY45GmzbZQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Kevinzz224\\AppData\\Local\\Temp\\ipykernel_43484\\787434732.py:16: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  answer = str(row[answer_col]).strip()\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('output.json', 83330, 2.707728862762451)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import uuid\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "def parse_row(row):\n",
        "    qas = []\n",
        "    for col in row.index:\n",
        "        if \"?\" in str(row[col]):\n",
        "            question = str(row[col]).strip()\n",
        "            answer_col = row.index.get_loc(col) + 1\n",
        "            if answer_col < len(row.index):\n",
        "                answer = str(row[answer_col]).strip()\n",
        "                qas.append((question, answer))\n",
        "    return qas\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"train.csv\", delimiter='\\t', on_bad_lines='skip', quoting=3)\n",
        "\n",
        "structured_data = []\n",
        "for idx, row in df.iterrows():\n",
        "    qas = parse_row(row)\n",
        "    for qa in qas:\n",
        "        structured_data.append({\n",
        "            \"id\": str(uuid.uuid4()),\n",
        "            \"Question\": qa[0],\n",
        "            \"Answer\": qa[1]\n",
        "        })\n",
        "\n",
        "output_file_path = 'output.json'\n",
        "with open(output_file_path, 'w') as outfile:\n",
        "    json.dump(structured_data, outfile, indent=2)\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "output_file_path, len(structured_data), execution_time"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
